{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "fs = 14\n",
    "sns.set_theme()\n",
    "                     \n",
    "\n",
    "metrics = ['Precision', 'Recall', 'F1', 'Balanced Accuracy', 'AUPRC', 'AUROC']\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "batchn = [True, False]\n",
    "for seed in [42, 43, 44, 45, 46]:  \n",
    "    for ba in batchn: \n",
    "\n",
    "        df = pd.read_csv(\"data/results/lstm/mimic/microbiology_res_False/ab_False/use_censored_True/lookback_7/aggregated_hours_4/seed_\"+str(seed)+\"/dropout_0-0/lambda_0-1/num_lin_layers_2/num_stacked_lstm_3/hidden_dim_256/lr_0-01/bs_128/is_tuned_False/use_relus_False/use_bn_\"+str(ba)+\"/test_res.csv\")\n",
    "        df.rename({'balanced_accuracy': 'Balanced Accuracy', 'prc_auc':'AUPRC', 'roc_auc':'AUROC', 'f1':'F1', 'recall':'Recall', 'precision':'Precision'}, inplace=True, axis=1)\n",
    "        df['Batch Normalization'] = ba\n",
    "\n",
    "        results_df = pd.concat([results_df, df])\n",
    "\n",
    "mean_std_df = results_df.groupby('Batch Normalization').agg({metric: ['mean', 'std'] for metric in metrics}).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "display(mean_std_df)\n",
    "\n",
    "\n",
    "for_latex = mean_std_df.set_index('Batch Normalization')\n",
    "#for_latex.index.rename('Batch Normalization', inplace=True)\n",
    "#for_latex = for_latex.transpose()\n",
    "display(for_latex)\n",
    "display(for_latex.transpose())\n",
    "print(for_latex.transpose().to_latex(float_format=\"%.2f\", bold_rows=True, caption='Performance metrics whether batchnormalization is used'))\n",
    "print(for_latex[['Balanced Accuracy','AUPRC','AUROC']].to_latex(float_format=\"%.2f\", bold_rows=True, caption='Performance metrics whether batchnormalization is used'))\n",
    "print(for_latex[['Precision','Recall','F1']].to_latex(float_format=\"%.2f\", bold_rows=True, caption='Performance metrics whether batchnormalization is used'))\n",
    "\n",
    "\n",
    "def plot_metric(metric):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.barplot(x='Batch Normalization', y=(metric, 'mean'), data=mean_std_df, capsize=.1, color='skyblue')\n",
    "    plt.errorbar(x=np.arange(len(batchn)), y=mean_std_df[(metric, 'mean')], yerr=mean_std_df[(metric, 'std')], fmt='none', c='black', capsize=5)\n",
    "    #plt.title(f'Mean and SD of {metric.capitalize()}')\n",
    "    plt.xlabel('Batch Normalization')\n",
    "    plt.ylabel(metric)\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xticks(fontsize=fs)\n",
    "    plt.yticks(fontsize=fs)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for metric in metrics:\n",
    "    plot_metric(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "                     \n",
    "metrics = ['Precision', 'Recall', 'F1', 'Balanced Accuracy', 'AUPRC', 'AUROC']\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "drop = [0.0, 0.3, 0.5]\n",
    "for seed in [42, 43, 44, 45, 46]:  \n",
    "    for dr in drop: \n",
    "                         \n",
    "        df = pd.read_csv(\"data/results/lstm/mimic/microbiology_res_False/ab_False/use_censored_True/lookback_7/aggregated_hours_4/seed_\"+str(seed)+\"/dropout_\"+str(dr).replace(\".\",\"-\")+\"/lambda_0-1/num_lin_layers_2/num_stacked_lstm_3/hidden_dim_256/lr_0-01/bs_128/is_tuned_False/use_relus_False/use_bn_False/test_res.csv\")\n",
    "        df.rename({'balanced_accuracy': 'Balanced Accuracy', 'prc_auc':'AUPRC', 'roc_auc':'AUROC', 'f1':'F1', 'recall':'Recall', 'precision':'Precision'}, inplace=True, axis=1)\n",
    "        df['Dropout'] = dr\n",
    "\n",
    "        results_df = pd.concat([results_df, df])\n",
    "\n",
    "mean_std_df = results_df.groupby('Dropout').agg({metric: ['mean', 'std'] for metric in metrics}).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "display(mean_std_df)\n",
    "\n",
    "for_latex = mean_std_df.set_index(('Dropout', ''))\n",
    "for_latex.index.rename('Dropout', inplace=True)\n",
    "for_latex = for_latex.transpose()\n",
    "display(for_latex)\n",
    "print(for_latex.to_latex(float_format=\"%.2f\", bold_rows=True, caption='Performance metrics whether dropout is used'))\n",
    "\n",
    "\n",
    "def plot_metric(metric):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.barplot(x='Dropout', y=(metric, 'mean'), data=mean_std_df, capsize=.1, color='skyblue')\n",
    "    plt.errorbar(x=np.arange(len(drop)), y=mean_std_df[(metric, 'mean')], yerr=mean_std_df[(metric, 'std')], fmt='none', c='black', capsize=5)\n",
    "    #plt.title(f'Mean and SD of {metric.capitalize()}')\n",
    "    plt.xlabel('Dropout')\n",
    "    plt.ylabel(metric)\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xticks(fontsize=fs)\n",
    "    plt.yticks(fontsize=fs)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for metric in metrics:\n",
    "    plot_metric(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sns.set_theme()\n",
    "                     \n",
    "\n",
    "\n",
    "\n",
    "metrics = ['Precision', 'Recall', 'F1', 'Balanced Accuracy', 'AUPRC', 'AUROC']\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "relu = [True, False]\n",
    "for seed in [42, 43, 44, 45, 46]:  \n",
    "    for re in relu: \n",
    "\n",
    "        df = pd.read_csv(\"data/results/lstm/mimic/microbiology_res_False/ab_False/use_censored_True/lookback_7/aggregated_hours_4/seed_\"+str(seed)+\"/dropout_0-0/lambda_0-1/num_lin_layers_2/num_stacked_lstm_3/hidden_dim_256/lr_0-01/bs_128/is_tuned_False/use_relus_\"+str(re)+\"/use_bn_False/test_res.csv\")\n",
    "        df.rename({'balanced_accuracy': 'Balanced Accuracy', 'prc_auc':'AUPRC', 'roc_auc':'AUROC', 'f1':'F1', 'recall':'Recall', 'precision':'Precision'}, inplace=True, axis=1)\n",
    "        df['ReLU'] = re\n",
    "\n",
    "        results_df = pd.concat([results_df, df])\n",
    "\n",
    "mean_std_df = results_df.groupby('ReLU').agg({metric: ['mean', 'std'] for metric in metrics}).reset_index()\n",
    "\n",
    "\n",
    "display(mean_std_df)\n",
    "\n",
    "\n",
    "for_latex = mean_std_df.set_index(('ReLU', ''))\n",
    "for_latex.index.rename('ReLU', inplace=True)\n",
    "for_latex = for_latex.transpose()\n",
    "display(for_latex)\n",
    "print(for_latex.to_latex(float_format=\"%.2f\", bold_rows=True, caption='Performance metrics whether RELUs are used'))\n",
    "\n",
    "def plot_metric(metric):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.barplot(x='ReLU', y=(metric, 'mean'), data=mean_std_df, capsize=.1, color='skyblue')\n",
    "    plt.errorbar(x=np.arange(len(relu)), y=mean_std_df[(metric, 'mean')], yerr=mean_std_df[(metric, 'std')], fmt='none', c='black', capsize=5)\n",
    "    #plt.title(f'Mean and SD of {metric.capitalize()}')\n",
    "    plt.xlabel('ReLU')\n",
    "    plt.ylabel(metric)\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xticks(fontsize=fs)\n",
    "    plt.yticks(fontsize=fs)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for metric in metrics:\n",
    "    plot_metric(metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
