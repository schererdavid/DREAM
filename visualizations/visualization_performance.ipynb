{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "fs = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIN=(0 1 2)\n",
    "# DROPOUT=(0 0.3 0.5)\n",
    "# BN=(True False)\n",
    "# RELU=(True False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database = 'mimic'\n",
    "\n",
    "# df = pd.DataFrame()\n",
    "\n",
    "# for ab in [False]: #True, \n",
    "#     for nsl in [3]: #, 4, 5, 1, 2\n",
    "#         for hd in [256]: #, 128, 512, 64\n",
    "#             for bs in [128]: #, 64, 32, 256\n",
    "#                 for la in [0.0]: #, 0.1, 0.01, 0.001, 0.0001, 0.00001\n",
    "#                     for lr in [0.01]: #, 0.1, 0.001, 0.0001\n",
    "#                         for lin in [0, 1, 2]:\n",
    "#                             for dropout in [0.0, 0.3, 0.5]:\n",
    "#                                 for bn in [True, False]:\n",
    "#                                     for relu in [True, False]:\n",
    "#                                         path = \"data/results/lstm/mimic/microbiology_res_False/ab_\"+str(ab)+\"/seed_42/dropout_\"+str(dropout).replace('.','-')+\"/lambda_\"+str(la).replace('.','-')+\"/num_lin_layers_\"+str(lin)+\"/num_stacked_lstm_\"+str(nsl)+\"/hidden_dim_\"+str(hd)+\"/lr_\"+str(lr).replace('.','-')+\"/bs_\"+str(bs)+\"/is_tuned_False/use_relus_\"+str(relu)+\"/use_bn_\"+str(bn)+\"/test_res.csv\"\n",
    "                                        \n",
    "#                                         if os.path.isfile(path):\n",
    "#                                             res = pd.read_csv(path)\n",
    "#                                             res['lin'] = lin\n",
    "#                                             res['dropout'] = dropout\n",
    "#                                             res['bn'] = bn\n",
    "#                                             res['relu'] = relu\n",
    "#                                             res['lr'] = lr\n",
    "#                                             res['lambda'] = la\n",
    "#                                             res['batch_size'] = bs\n",
    "#                                             res['hidden_dim'] = hd\n",
    "#                                             res['number_stacked_layers'] = nsl\n",
    "#                                             res['ab_features'] = ab\n",
    "#                                             df = pd.concat([df, res], axis=0)\n",
    "# print(df.shape)\n",
    "# display(df.sort_values(['balanced_accuracy'], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame()\n",
    "\n",
    "# for ab in [False]: #, True\n",
    "#     for nsl in [3, 4, 5, 1, 2]: #\n",
    "#         for hd in [256, 128, 512, 64]: #\n",
    "#             for bs in [128, 64, 32, 256, ]: #\n",
    "#                 for la in [0.0, 0.1, 0.01, 0.001, 0.0001, 0.00001]: # , 0.1, 0.01, 0.001, 0.0001, 0.00001\n",
    "#                     for lr in [0.01]: #, 0.1, 0.001, 0.0001\n",
    "#                         for lin in [0]: #, 1, 2\n",
    "#                             for dropout in [0.0]: #, 0.3, 0.5\n",
    "#                                 for bn in [False]: #True, \n",
    "#                                     for relu in [False]: #True, \n",
    "#                                         path = \"data/results/lstm/mimic/microbiology_res_False/ab_\"+str(ab)+\"/seed_42/dropout_\"+str(dropout).replace('.','-')+\"/lambda_\"+str(la).replace('.','-')+\"/num_lin_layers_\"+str(lin)+\"/num_stacked_lstm_\"+str(nsl)+\"/hidden_dim_\"+str(hd)+\"/lr_\"+str(lr).replace('.','-')+\"/bs_\"+str(bs)+\"/is_tuned_False/use_relus_\"+str(relu)+\"/use_bn_\"+str(bn)+\"/test_res.csv\"\n",
    "                                        \n",
    "#                                         if os.path.isfile(path):\n",
    "#                                             res = pd.read_csv(path)\n",
    "#                                             res['lin'] = lin\n",
    "#                                             res['dropout'] = dropout\n",
    "#                                             res['bn'] = bn\n",
    "#                                             res['relu'] = relu\n",
    "#                                             res['lr'] = lr\n",
    "#                                             res['lambda'] = la\n",
    "#                                             res['batch_size'] = bs\n",
    "#                                             res['hidden_dim'] = hd\n",
    "#                                             res['number_stacked_layers'] = nsl\n",
    "#                                             res['ab_features'] = ab\n",
    "#                                             df = pd.concat([df, res], axis=0)\n",
    "# print(df.shape)\n",
    "# display(df.sort_values(['balanced_accuracy'], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# sbatch -c 4 -p gpu --gres=gpu:1 --mem-per-cpu=32G --time=1:00:00 --wrap \"python3 main.py -t lstm_train_test_evaluate -d mimic -ab True -m False -la 0 -s 42 -f False -nl 0 -dr 0 -lr 0.01 -it False -bs 128 -hd 256 -nsl 3 -bn False -re False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame()\n",
    "\n",
    "# for ab in [False]:\n",
    "#     for nsl in [4, 5]:\n",
    "#         for hd in [128]: \n",
    "#             for bs in [256]:\n",
    "#                 for la in [0.1, 0.2, 0.09]:\n",
    "#                     for lr in [0.01]:\n",
    "#                         for lin in [1, 2]:\n",
    "#                             for dropout in [0.4, 0.5]: \n",
    "#                                 for bn in [True, False]: \n",
    "#                                     for relu in [True, False]: \n",
    "#                                         path = \"data/results/lstm/mimic/microbiology_res_False/ab_\"+str(ab)+\"/seed_42/dropout_\"+str(dropout).replace('.','-')+\"/lambda_\"+str(la).replace('.','-')+\"/num_lin_layers_\"+str(lin)+\"/num_stacked_lstm_\"+str(nsl)+\"/hidden_dim_\"+str(hd)+\"/lr_\"+str(lr).replace('.','-')+\"/bs_\"+str(bs)+\"/is_tuned_False/use_relus_\"+str(relu)+\"/use_bn_\"+str(bn)+\"/test_res.csv\"\n",
    "                                        \n",
    "#                                         if os.path.isfile(path):\n",
    "#                                             res = pd.read_csv(path)\n",
    "#                                             res['lin'] = lin\n",
    "#                                             res['dropout'] = dropout\n",
    "#                                             res['bn'] = bn\n",
    "#                                             res['relu'] = relu\n",
    "#                                             res['lr'] = lr\n",
    "#                                             res['lambda'] = la\n",
    "#                                             res['batch_size'] = bs\n",
    "#                                             res['hidden_dim'] = hd\n",
    "#                                             res['number_stacked_layers'] = nsl\n",
    "#                                             res['ab_features'] = ab\n",
    "#                                             df = pd.concat([df, res], axis=0)\n",
    "# print(df.shape)\n",
    "# display(df.sort_values(['balanced_accuracy'], ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "microbio true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame()\n",
    "\n",
    "# LIN=(0 1 2)\n",
    "# DROPOUT=(0.4 0.5)\n",
    "# BN=(True)\n",
    "# RELU=(True)\n",
    "# LR=(0.01)\n",
    "# LA=(0.1 0, 0.01, 0.001 0.0001 0.00001)\n",
    "# BS=(256)\n",
    "# HD=(128)\n",
    "# NSL=(4)\n",
    "\n",
    "# for ab in [False]:\n",
    "#     for nsl in [4]:\n",
    "#         for hd in [128]: \n",
    "#             for bs in [256]:\n",
    "#                 for la in [0.1, 0, 0.01, 0.001, 0.0001, 0.00001]:\n",
    "#                     for lr in [0.01]:\n",
    "#                         for lin in [0, 1, 2]:\n",
    "#                             for dropout in [0.4, 0.5]: \n",
    "#                                 for bn in [True]: \n",
    "#                                     for relu in [True]: \n",
    "#                                         path = \"data/results/lstm/mimic/microbiology_res_True/ab_\"+str(ab)+\"/seed_42/dropout_\"+str(dropout).replace('.','-')+\"/lambda_\"+str(la).replace('.','-')+\"/num_lin_layers_\"+str(lin)+\"/num_stacked_lstm_\"+str(nsl)+\"/hidden_dim_\"+str(hd)+\"/lr_\"+str(lr).replace('.','-')+\"/bs_\"+str(bs)+\"/is_tuned_False/use_relus_\"+str(relu)+\"/use_bn_\"+str(bn)+\"/test_res.csv\"\n",
    "                                        \n",
    "#                                         if os.path.isfile(path):\n",
    "#                                             res = pd.read_csv(path)\n",
    "#                                             res['lin'] = lin\n",
    "#                                             res['dropout'] = dropout\n",
    "#                                             res['bn'] = bn\n",
    "#                                             res['relu'] = relu\n",
    "#                                             res['lr'] = lr\n",
    "#                                             res['lambda'] = la\n",
    "#                                             res['batch_size'] = bs\n",
    "#                                             res['hidden_dim'] = hd\n",
    "#                                             res['number_stacked_layers'] = nsl\n",
    "#                                             res['ab_features'] = ab\n",
    "#                                             df = pd.concat([df, res], axis=0)\n",
    "# print(df.shape)\n",
    "# display(df.sort_values(['balanced_accuracy'], ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# traditional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### microbiologie false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "fs = 14\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "\n",
    "base_path = \"data/results/traditional/mimic/microbiology_res_False/ab_False/\"\n",
    "seeds = [42, 43, 44, 45 , 46] #, 46\n",
    "models = ['LGBMClassifier', 'LogisticRegression', 'MLPClassifier']\n",
    "\n",
    "\n",
    "roc_data = {model: {'mean_fpr': np.linspace(0, 1, 100), 'tpr_list': [], 'auc_list': []} for model in models}\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    for seed in seeds:\n",
    "        file_path = f\"{base_path}seed_{seed}/lookback_2/time_point('random', 1)/sample_None_None/{model}/test_gt_and_preds.csv\"\n",
    "        data = pd.read_csv(file_path)\n",
    "        gt = data['lot<5d']\n",
    "        pred_prob = data['True']\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(gt, pred_prob)\n",
    "        roc_data[model]['tpr_list'].append(np.interp(roc_data[model]['mean_fpr'], fpr, tpr))\n",
    "        roc_data[model]['tpr_list'][-1][0] = 0.0  # Startpunkt bei 0\n",
    "        roc_data[model]['auc_list'].append(auc(fpr, tpr)) \n",
    "\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    mean_tpr = np.mean(roc_data[model]['tpr_list'], axis=0)\n",
    "    mean_tpr[-1] = 1.0  \n",
    "    mean_auc = np.mean(roc_data[model]['auc_list'])\n",
    "    std_auc = np.std(roc_data[model]['auc_list']) \n",
    "    std_tpr = np.std(roc_data[model]['tpr_list'], axis=0)\n",
    "\n",
    "    plt.plot(roc_data[model]['mean_fpr'], mean_tpr, label=f'{model} (AUC = {mean_auc:.2f} ± {std_auc:.2f})')\n",
    "    \n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    plt.fill_between(roc_data[model]['mean_fpr'], tprs_lower, tprs_upper, alpha=0.2)\n",
    "\n",
    "\n",
    "fs = 14\n",
    "plt.legend(loc='lower right')\n",
    "plt.legend(fontsize=fs)\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate', fontsize=fs)\n",
    "plt.xlabel('False Positive Rate', fontsize=fs)\n",
    "plt.xticks(fontsize=fs)\n",
    "plt.yticks(fontsize=fs)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/experiments/general/traditional_performance_auroc.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pr_data = {model: {'recall': np.linspace(0, 1, 100), 'precision_list': [], 'ap_list': []} for model in models}\n",
    "\n",
    "positive_class_frequency = 0\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    for seed in seeds:\n",
    "        file_path = f\"{base_path}seed_{seed}/lookback_2/time_point('random', 1)/sample_None_None/{model}/test_gt_and_preds.csv\"\n",
    "        data = pd.read_csv(file_path)\n",
    "        gt = data['lot<5d']\n",
    "        pred_prob = data['True']\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(gt, pred_prob)\n",
    "        pr_data[model]['precision_list'].append(np.interp(pr_data[model]['recall'], recall[::-1], precision[::-1]))\n",
    "        pr_data[model]['ap_list'].append(average_precision_score(gt, pred_prob))\n",
    "        positive_class_frequency += gt.mean()\n",
    "\n",
    "\n",
    "positive_class_frequency /= len(seeds) * len(models)\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    mean_precision = np.mean(pr_data[model]['precision_list'], axis=0)\n",
    "    mean_ap = np.mean(pr_data[model]['ap_list'])\n",
    "    std_ap = np.std(pr_data[model]['ap_list'])\n",
    "    std_precision = np.std(pr_data[model]['precision_list'], axis=0)\n",
    "    plt.plot(pr_data[model]['recall'], mean_precision, label=f'{model} (AP = {mean_ap:.2f} ± {std_ap:.2f})')\n",
    "    precision_upper = np.minimum(mean_precision + std_precision, 1)\n",
    "    precision_lower = np.maximum(mean_precision - std_precision, 0)\n",
    "    plt.fill_between(pr_data[model]['recall'], precision_lower, precision_upper, alpha=0.2)\n",
    "\n",
    "fs = 14\n",
    "plt.hlines(positive_class_frequency, 0, 1, colors='red', linestyles='dashed', label='chance level')\n",
    "plt.legend(loc='lower left')\n",
    "plt.legend(fontsize=fs)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('Precision', fontsize=fs)\n",
    "plt.xlabel('Recall', fontsize=fs)\n",
    "plt.xticks(fontsize=fs)\n",
    "plt.yticks(fontsize=fs)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/experiments/general/traditional_performance_auprc.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_measurements = pd.DataFrame()\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    for seed in seeds:\n",
    "        file_path = f\"data/results/traditional/mimic/microbiology_res_False/ab_False/seed_{seed}/lookback_2/time_point('random', 1)/sample_None_None/{model}/test_res.csv\"\n",
    "        measurements = pd.read_csv(file_path).drop([\"Unnamed: 0\"], axis=1)\n",
    "        measurements['model'] = model\n",
    "        measurements['seed'] = seed\n",
    "        all_measurements = pd.concat([all_measurements, measurements], ignore_index=True) \n",
    "all_measurements.rename({'balanced_accuracy': 'Balanced Accuracy', 'prc_auc':'AUPRC', 'roc_auc':'AUROC', 'f1':'F1', 'recall':'Recall', 'precision':'Precision'}, inplace=True, axis=1)\n",
    "mean_measurements = all_measurements.groupby('model').mean().reset_index()\n",
    "std_measurements = all_measurements.groupby('model').std().reset_index()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mean_melted = mean_measurements.melt(id_vars='model', var_name='metric', value_name='mean')\n",
    "std_melted = std_measurements.melt(id_vars='model', var_name='metric', value_name='std')\n",
    "\n",
    "\n",
    "merged_measurements = pd.merge(mean_melted, std_melted, on=['model', 'metric'])\n",
    "\n",
    "display(merged_measurements)\n",
    "\n",
    "for metric in merged_measurements['metric'].unique():\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    metric_data = merged_measurements[merged_measurements['metric'] == metric]\n",
    "    display(metric_data)\n",
    "    sns.barplot(data=metric_data, x='model', y='mean',capsize=.1)\n",
    "    plt.errorbar(data=metric_data, x=range(len(metric_data)), y='mean', yerr=metric_data['std'], fmt='none', c='black', capsize=5)\n",
    "    #plt.title(f'{metric}')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xticks(fontsize=fs)\n",
    "    plt.yticks(fontsize=fs)\n",
    "    plt.ylabel(metric, fontsize=fs)\n",
    "    plt.xlabel('', fontsize=fs)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_measurements = all_measurements.groupby('model').mean().reset_index()\n",
    "std_measurements = all_measurements.groupby('model').std().reset_index()\n",
    "metrics = mean_measurements.columns.difference(['model', 'seed'])\n",
    "df_combined = pd.DataFrame(index=mean_measurements['model'], columns=pd.MultiIndex.from_product([metrics, ['mean', 'std']]))\n",
    "for metric in metrics:\n",
    "    df_combined[(metric, 'mean')] = mean_measurements.set_index('model')[metric]\n",
    "    df_combined[(metric, 'std')] = std_measurements.set_index('model')[metric]\n",
    "\n",
    "df_combined.reset_index() \n",
    "display(df_combined)\n",
    "print(df_combined[['Balanced Accuracy', 'AUPRC', 'AUROC']].to_latex(float_format=\"%.2f\", bold_rows=True, caption='Performance of different traditional models'))\n",
    "print(df_combined[['Precision', 'Recall', 'F1']].to_latex(float_format=\"%.2f\", bold_rows=True, caption='Performance of different traditional models'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### microbiologie true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "\n",
    "base_path = \"data/results/traditional/mimic/microbiology_res_True/ab_False/\"\n",
    "seeds = [42, 43, 44, 45, 46] #\n",
    "models = ['LGBMClassifier', 'LogisticRegression', 'MLPClassifier']\n",
    "\n",
    "\n",
    "roc_data = {model: {'mean_fpr': np.linspace(0, 1, 100), 'tpr_list': [], 'auc_list': []} for model in models}\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    for seed in seeds:\n",
    "        file_path = f\"{base_path}seed_{seed}/lookback_2/time_point('random', 1)/sample_None_None/{model}/test_gt_and_preds.csv\"\n",
    "        data = pd.read_csv(file_path)\n",
    "        gt = data['lot<5d']\n",
    "        pred_prob = data['True']\n",
    "\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(gt, pred_prob)\n",
    "        roc_data[model]['tpr_list'].append(np.interp(roc_data[model]['mean_fpr'], fpr, tpr))\n",
    "        roc_data[model]['tpr_list'][-1][0] = 0.0  \n",
    "        roc_data[model]['auc_list'].append(auc(fpr, tpr)) \n",
    "\n",
    "\n",
    "for model in models:\n",
    "    mean_tpr = np.mean(roc_data[model]['tpr_list'], axis=0)\n",
    "    mean_tpr[-1] = 1.0  \n",
    "    mean_auc = np.mean(roc_data[model]['auc_list'])\n",
    "    std_auc = np.std(roc_data[model]['auc_list']) \n",
    "    std_tpr = np.std(roc_data[model]['tpr_list'], axis=0)\n",
    "\n",
    "    plt.plot(roc_data[model]['mean_fpr'], mean_tpr, label=f'{model} (AUC = {mean_auc:.2f} ± {std_auc:.2f})')\n",
    "    \n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    plt.fill_between(roc_data[model]['mean_fpr'], tprs_lower, tprs_upper, alpha=0.2)\n",
    "\n",
    "plt.title('Durchschnittliche Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.legend(fontsize=fs)\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate', fontsize=fs)\n",
    "plt.xlabel('False Positive Rate', fontsize=fs)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "pr_data = {model: {'recall': np.linspace(0, 1, 100), 'precision_list': [], 'ap_list': []} for model in models}\n",
    "\n",
    "\n",
    "positive_class_frequency = 0\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    for seed in seeds:\n",
    "        file_path = f\"{base_path}seed_{seed}/lookback_2/time_point('random', 1)/sample_None_None/{model}/test_gt_and_preds.csv\"\n",
    "        data = pd.read_csv(file_path)\n",
    "        gt = data['lot<5d']\n",
    "        pred_prob = data['True']\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(gt, pred_prob)\n",
    "        pr_data[model]['precision_list'].append(np.interp(pr_data[model]['recall'], recall[::-1], precision[::-1]))\n",
    "        pr_data[model]['ap_list'].append(average_precision_score(gt, pred_prob))\n",
    "        positive_class_frequency += gt.mean()\n",
    "\n",
    "positive_class_frequency /= len(seeds) * len(models)\n",
    "\n",
    "for model in models:\n",
    "    mean_precision = np.mean(pr_data[model]['precision_list'], axis=0)\n",
    "    mean_ap = np.mean(pr_data[model]['ap_list'])\n",
    "    std_ap = np.std(pr_data[model]['ap_list'])\n",
    "    std_precision = np.std(pr_data[model]['precision_list'], axis=0)\n",
    "\n",
    "    plt.plot(pr_data[model]['recall'], mean_precision, label=f'{model} (AP = {mean_ap:.2f} ± {std_ap:.2f})')\n",
    "    \n",
    "    precision_upper = np.minimum(mean_precision + std_precision, 1)\n",
    "    precision_lower = np.maximum(mean_precision - std_precision, 0)\n",
    "    plt.fill_between(pr_data[model]['recall'], precision_lower, precision_upper, alpha=0.2)\n",
    "\n",
    "\n",
    "plt.hlines(positive_class_frequency, 0, 1, colors='red', linestyles='dashed', label='Random')\n",
    "\n",
    "plt.title('Durchschnittliche Precision-Recall-Kurve')\n",
    "plt.legend(loc='lower left')\n",
    "plt.legend(fontsize=fs)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('Precision', fontsize=fs)\n",
    "plt.xlabel('Recall', fontsize=fs)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_measurements = pd.DataFrame()\n",
    "\n",
    "# Load the data for each model and seed\n",
    "for model in models:\n",
    "    for seed in seeds:\n",
    "        # Construct the file path\n",
    "        file_path = f\"data/results/traditional/mimic/microbiology_res_True/ab_False/seed_{seed}/lookback_2/time_point('random', 1)/sample_None_None/{model}/test_res.csv\"\n",
    "        \n",
    "        # Load the data and drop the unnecessary column\n",
    "        measurements = pd.read_csv(file_path).drop([\"Unnamed: 0\"], axis=1)\n",
    "        \n",
    "        # Add a column to identify the model and seed\n",
    "        measurements['model'] = model\n",
    "        measurements['seed'] = seed\n",
    "        \n",
    "        # Append the measurements to the all_measurements DataFrame\n",
    "        all_measurements = pd.concat([all_measurements, measurements], ignore_index=True) #all_measurements.append(measurements, ignore_index=True)\n",
    "\n",
    "# Compute the mean and standard deviation for each metric across all models and seeds\n",
    "mean_measurements = all_measurements.groupby('model').mean().reset_index()\n",
    "std_measurements = all_measurements.groupby('model').std().reset_index()\n",
    "\n",
    "# Melt the DataFrames to have the metric names as a single column, which is required for seaborn plotting\n",
    "mean_melted = mean_measurements.melt(id_vars='model', var_name='metric', value_name='mean')\n",
    "std_melted = std_measurements.melt(id_vars='model', var_name='metric', value_name='std')\n",
    "\n",
    "# Merge the mean and std DataFrames on model and metric to have a single DataFrame for plotting\n",
    "merged_measurements = pd.merge(mean_melted, std_melted, on=['model', 'metric'])\n",
    "\n",
    "# Loop over each metric and create a separate plot for it\n",
    "for metric in merged_measurements['metric'].unique():\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    metric_data = merged_measurements[merged_measurements['metric'] == metric]\n",
    "    display(metric_data)\n",
    "    sns.barplot(data=metric_data, x='model', y='mean',capsize=.1)\n",
    "    plt.errorbar(data=metric_data, x=range(len(metric_data)), y='mean', yerr=metric_data['std'], fmt='none', c='black', capsize=5)\n",
    "    plt.title(f'{metric}')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('Mean Value and Standard Deviation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# next day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### microbio false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "\n",
    "base_path = \"data/results/lstm/mimic/microbiology_res_False/ab_False/use_censored_True/lookback_7/aggregated_hours_4/\"\n",
    "seeds = [42, 43, 44, 45 , 46]\n",
    "models = ['Next Day']\n",
    "\n",
    "\n",
    "roc_data = {model: {'mean_fpr': np.linspace(0, 1, 100), 'tpr_list': [], 'auc_list': []} for model in models}\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    for seed in seeds:\n",
    "        file_path = f\"{base_path}seed_{seed}/dropout_0-0/lambda_0-1/num_lin_layers_2/num_stacked_lstm_3/hidden_dim_256/lr_0-01/bs_128/is_tuned_False/use_relus_False/use_bn_False/test_gt_and_preds.csv\"\n",
    "        data = pd.read_csv(file_path)\n",
    "        gt = data['next_day']\n",
    "        pred_prob = data['True']\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(gt, pred_prob)\n",
    "        roc_data[model]['tpr_list'].append(np.interp(roc_data[model]['mean_fpr'], fpr, tpr))\n",
    "        roc_data[model]['tpr_list'][-1][0] = 0.0  # Startpunkt bei 0\n",
    "        roc_data[model]['auc_list'].append(auc(fpr, tpr)) \n",
    "\n",
    "\n",
    "for model in models:\n",
    "    mean_tpr = np.mean(roc_data[model]['tpr_list'], axis=0)\n",
    "    mean_tpr[-1] = 1.0  \n",
    "    mean_auc = np.mean(roc_data[model]['auc_list'])\n",
    "    std_auc = np.std(roc_data[model]['auc_list']) \n",
    "    std_tpr = np.std(roc_data[model]['tpr_list'], axis=0)\n",
    "\n",
    "    plt.plot(roc_data[model]['mean_fpr'], mean_tpr, label=f'{model} (AUC = {mean_auc:.2f} ± {std_auc:.2f})')\n",
    "    \n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    plt.fill_between(roc_data[model]['mean_fpr'], tprs_lower, tprs_upper, alpha=0.2)\n",
    "\n",
    "#plt.title('Durchschnittliche Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.legend(fontsize=fs)\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate', fontsize=fs)\n",
    "plt.xlabel('False Positive Rate', fontsize=fs)\n",
    "plt.xticks(fontsize=fs)\n",
    "plt.yticks(fontsize=fs)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/experiments/general/nd_performance_auroc.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pr_data = {model: {'recall': np.linspace(0, 1, 100), 'precision_list': [], 'ap_list': []} for model in models}\n",
    "\n",
    "positive_class_frequency = 0\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    for seed in seeds:\n",
    "        file_path = f\"{base_path}seed_{seed}/dropout_0-0/lambda_0-1/num_lin_layers_2/num_stacked_lstm_3/hidden_dim_256/lr_0-01/bs_128/is_tuned_False/use_relus_False/use_bn_False/test_gt_and_preds.csv\"\n",
    "        data = pd.read_csv(file_path)\n",
    "        gt = data['next_day']\n",
    "        pred_prob = data['True']\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(gt, pred_prob)\n",
    "        pr_data[model]['precision_list'].append(np.interp(pr_data[model]['recall'], recall[::-1], precision[::-1]))\n",
    "        pr_data[model]['ap_list'].append(average_precision_score(gt, pred_prob))\n",
    "        positive_class_frequency += gt.mean()\n",
    "\n",
    "\n",
    "positive_class_frequency /= len(seeds) * len(models)\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    mean_precision = np.mean(pr_data[model]['precision_list'], axis=0)\n",
    "    mean_ap = np.mean(pr_data[model]['ap_list'])\n",
    "    std_ap = np.std(pr_data[model]['ap_list'])\n",
    "    std_precision = np.std(pr_data[model]['precision_list'], axis=0)\n",
    "\n",
    "    plt.plot(pr_data[model]['recall'], mean_precision, label=f'{model} (AP = {mean_ap:.2f} ± {std_ap:.2f})')\n",
    "    \n",
    "    precision_upper = np.minimum(mean_precision + std_precision, 1)\n",
    "    precision_lower = np.maximum(mean_precision - std_precision, 0)\n",
    "    plt.fill_between(pr_data[model]['recall'], precision_lower, precision_upper, alpha=0.2)\n",
    "\n",
    "plt.hlines(positive_class_frequency, 0, 1, colors='red', linestyles='dashed', label='chance level')\n",
    "\n",
    "\n",
    "plt.legend(loc='lower left')\n",
    "plt.legend(fontsize=fs)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('Precision', fontsize=fs)\n",
    "plt.xlabel('Recall', fontsize=fs)\n",
    "plt.xticks(fontsize=fs)\n",
    "plt.yticks(fontsize=fs)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/experiments/general/nd_performance_auprc.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_measurements = pd.DataFrame()\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    for seed in seeds:\n",
    "        file_path = f\"{base_path}seed_{seed}/dropout_0-0/lambda_0-1/num_lin_layers_2/num_stacked_lstm_3/hidden_dim_256/lr_0-01/bs_128/is_tuned_False/use_relus_False/use_bn_False/test_res.csv\"\n",
    "        measurements = pd.read_csv(file_path).drop([\"Unnamed: 0\"], axis=1)\n",
    "        measurements.rename({'balanced_accuracy': 'Balanced Accuracy', 'prc_auc':'AUPRC', 'roc_auc':'AUROC', 'f1':'F1', 'recall':'Recall', 'precision':'Precision'}, inplace=True, axis=1)\n",
    "        measurements['model'] = model\n",
    "        measurements['seed'] = seed\n",
    "        all_measurements = pd.concat([all_measurements, measurements], ignore_index=True) \n",
    "\n",
    "metrics = ['Precision', 'Recall', 'F1', 'Balanced Accuracy', 'AUPRC', 'AUROC']\n",
    "mean_std_df = all_measurements.groupby('model').agg({metric: ['mean', 'std'] for metric in metrics}).reset_index()\n",
    "\n",
    "\n",
    "for_latex = mean_std_df.set_index(('model', ''))\n",
    "for_latex.index.rename('model', inplace=True)\n",
    "display(for_latex)\n",
    "\n",
    "print(for_latex[['Balanced Accuracy', 'AUPRC', 'AUROC']].to_latex(float_format=\"%.2f\", bold_rows=True, caption='Next day model'))\n",
    "print(for_latex[['Precision', 'Recall', 'F1']].to_latex(float_format=\"%.2f\", bold_rows=True, caption='Next day model'))\n",
    "\n",
    "\n",
    "\n",
    "#display(all_measurements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### microbio true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "\n",
    "base_path = \"data/results/lstm/mimic/microbiology_res_True/ab_False/use_censored_True/lookback_7/aggregated_hours_4/\"\n",
    "seeds = [42, 43, 44, 45 , 46]\n",
    "models = ['Next Day']\n",
    "\n",
    "\n",
    "roc_data = {model: {'mean_fpr': np.linspace(0, 1, 100), 'tpr_list': [], 'auc_list': []} for model in models}\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    for seed in seeds:\n",
    "        file_path = f\"{base_path}seed_{seed}/dropout_0-0/lambda_0-1/num_lin_layers_2/num_stacked_lstm_3/hidden_dim_256/lr_0-01/bs_128/is_tuned_False/use_relus_False/use_bn_False/test_gt_and_preds.csv\"\n",
    "        data = pd.read_csv(file_path)\n",
    "        gt = data['next_day']\n",
    "        pred_prob = data['True']\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(gt, pred_prob)\n",
    "        roc_data[model]['tpr_list'].append(np.interp(roc_data[model]['mean_fpr'], fpr, tpr))\n",
    "        roc_data[model]['tpr_list'][-1][0] = 0.0  # Startpunkt bei 0\n",
    "        roc_data[model]['auc_list'].append(auc(fpr, tpr)) \n",
    "\n",
    "\n",
    "for model in models:\n",
    "    mean_tpr = np.mean(roc_data[model]['tpr_list'], axis=0)\n",
    "    mean_tpr[-1] = 1.0  \n",
    "    mean_auc = np.mean(roc_data[model]['auc_list'])\n",
    "    std_auc = np.std(roc_data[model]['auc_list']) \n",
    "    std_tpr = np.std(roc_data[model]['tpr_list'], axis=0)\n",
    "\n",
    "    plt.plot(roc_data[model]['mean_fpr'], mean_tpr, label=f'{model} (AUC = {mean_auc:.2f} ± {std_auc:.2f})')\n",
    "    \n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    plt.fill_between(roc_data[model]['mean_fpr'], tprs_lower, tprs_upper, alpha=0.2)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.legend(fontsize=fs)\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate', fontsize=fs)\n",
    "plt.xlabel('False Positive Rate', fontsize=fs)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pr_data = {model: {'recall': np.linspace(0, 1, 100), 'precision_list': [], 'ap_list': []} for model in models}\n",
    "\n",
    "positive_class_frequency = 0\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    for seed in seeds:\n",
    "        file_path = f\"{base_path}seed_{seed}/dropout_0-0/lambda_0-1/num_lin_layers_2/num_stacked_lstm_3/hidden_dim_256/lr_0-01/bs_128/is_tuned_False/use_relus_False/use_bn_False/test_gt_and_preds.csv\"\n",
    "        data = pd.read_csv(file_path)\n",
    "        gt = data['next_day']\n",
    "        pred_prob = data['True']\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(gt, pred_prob)\n",
    "        pr_data[model]['precision_list'].append(np.interp(pr_data[model]['recall'], recall[::-1], precision[::-1]))\n",
    "        pr_data[model]['ap_list'].append(average_precision_score(gt, pred_prob))\n",
    "        positive_class_frequency += gt.mean()\n",
    "\n",
    "\n",
    "positive_class_frequency /= len(seeds) * len(models)\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    mean_precision = np.mean(pr_data[model]['precision_list'], axis=0)\n",
    "    mean_ap = np.mean(pr_data[model]['ap_list'])\n",
    "    std_ap = np.std(pr_data[model]['ap_list'])\n",
    "    std_precision = np.std(pr_data[model]['precision_list'], axis=0)\n",
    "\n",
    "    plt.plot(pr_data[model]['recall'], mean_precision, label=f'{model} (AP = {mean_ap:.2f} ± {std_ap:.2f})')\n",
    "    \n",
    "    precision_upper = np.minimum(mean_precision + std_precision, 1)\n",
    "    precision_lower = np.maximum(mean_precision - std_precision, 0)\n",
    "    plt.fill_between(pr_data[model]['recall'], precision_lower, precision_upper, alpha=0.2)\n",
    "\n",
    "\n",
    "plt.hlines(positive_class_frequency, 0, 1, colors='red', linestyles='dashed', label='Random')\n",
    "\n",
    "plt.title('Durchschnittliche Precision-Recall-Kurve')\n",
    "plt.legend(loc='lower left')\n",
    "plt.legend(fontsize=fs)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('Precision', fontsize=fs)\n",
    "plt.xlabel('Recall', fontsize=fs)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_measurements = pd.DataFrame()\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    for seed in seeds:\n",
    "        file_path = f\"{base_path}seed_{seed}/dropout_0-0/lambda_0-1/num_lin_layers_2/num_stacked_lstm_3/hidden_dim_256/lr_0-01/bs_128/is_tuned_False/use_relus_False/use_bn_False/test_res.csv\"\n",
    "        measurements = pd.read_csv(file_path).drop([\"Unnamed: 0\"], axis=1)\n",
    "        measurements.rename({'balanced_accuracy': 'Balanced Accuracy', 'prc_auc':'AUPRC', 'roc_auc':'AUROC', 'f1':'F1', 'recall':'Recall', 'precision':'Precision'}, inplace=True, axis=1)\n",
    "        measurements['model'] = model\n",
    "        measurements['seed'] = seed\n",
    "        all_measurements = pd.concat([all_measurements, measurements], ignore_index=True) \n",
    "\n",
    "metrics = ['Precision', 'Recall', 'F1', 'Balanced Accuracy', 'AUPRC', 'AUROC']\n",
    "mean_std_df = all_measurements.groupby('model').agg({metric: ['mean', 'std'] for metric in metrics}).reset_index()\n",
    "\n",
    "\n",
    "for_latex = mean_std_df.set_index(('model', ''))\n",
    "for_latex.index.rename('model', inplace=True)\n",
    "display(for_latex)\n",
    "\n",
    "print(for_latex[['Balanced Accuracy', 'AUPRC', 'AUROC']].to_latex(float_format=\"%.2f\", bold_rows=True, caption='Next day model'))\n",
    "print(for_latex[['Precision', 'Recall', 'F1']].to_latex(float_format=\"%.2f\", bold_rows=True, caption='Next day model'))\n",
    "\n",
    "\n",
    "\n",
    "# for metric in merged_measurements['metric'].unique():\n",
    "#     plt.figure(figsize=(7, 5))\n",
    "#     metric_data = merged_measurements[merged_measurements['metric'] == metric]\n",
    "#     display(metric_data)\n",
    "#     sns.barplot(data=metric_data, x='model', y='mean',capsize=.1)\n",
    "#     plt.errorbar(data=metric_data, x=range(len(metric_data)), y='mean', yerr=metric_data['std'], fmt='none', c='black', capsize=5)\n",
    "#     plt.title(f'{metric}')\n",
    "#     plt.ylim([0, 1])\n",
    "#     plt.ylabel('Mean Value and Standard Deviation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
