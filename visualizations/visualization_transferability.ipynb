{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import PrecisionRecallDisplay, RocCurveDisplay\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.helpers import get_res_df, get_standard_stats\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import seaborn as sns\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def independent_test(database='eicu', \n",
    "                  lookback=2, \n",
    "                  prediction_time_points='random', \n",
    "                  numberofsamples=1, \n",
    "                  sample_train=None, \n",
    "                  sample_test=None, \n",
    "                  seed=44, \n",
    "                  inc_ab=False,\n",
    "                  has_microbiology=False,\n",
    "                  model='LGBMClassifier',):\n",
    "    if prediction_time_points == 'random':\n",
    "        time_point = ('random', numberofsamples)\n",
    "\n",
    "    original_traditional_path = 'data/model_input/traditional/mimic/microbiology_res_'+str(has_microbiology)+'/ab_'+str(inc_ab)+'/seed_'+str(seed)+'/'\n",
    "\n",
    "    independent_traditional_path = 'data/model_input/traditional/'+database+'/microbiology_res_'+str(has_microbiology)+'/ab_'+str(inc_ab)+'/seed_'+str(seed)+'/'\n",
    "\n",
    "    traditional_model_path = 'data/results/traditional/mimic/microbiology_res_'+str(has_microbiology)+'/ab_'+str(inc_ab)+'/seed_'+str(seed)+ \\\n",
    "                            '/lookback_'+str(lookback)+'/time_point'+str(time_point)+'/sample_'+str(sample_train)+\"_\"+str(sample_test)+\"/\"+model+\"/\"\n",
    "\n",
    "    # first we get the complete test dataset for the traditional model\n",
    "    if database != 'mimic':\n",
    "        X_traditional_independent = pd.read_parquet(independent_traditional_path+'X_train_time_point_'+str(time_point)+'_lookback_'+str(lookback)+'.parquet')\n",
    "        y_traditional_independent = pd.read_parquet(independent_traditional_path+'y_train_time_point_'+str(time_point)+'_lookback_'+str(lookback)+'.parquet')\n",
    "    else:\n",
    "        X_traditional_independent = pd.DataFrame()\n",
    "        y_traditional_independent = pd.DataFrame()\n",
    "    X_traditional_independent_test = pd.read_parquet(independent_traditional_path+'X_test_time_point_'+str(time_point)+'_lookback_'+str(lookback)+'.parquet')\n",
    "    y_traditional_independent_test = pd.read_parquet(independent_traditional_path+'y_test_time_point_'+str(time_point)+'_lookback_'+str(lookback)+'.parquet')\n",
    "    X_traditional_independent = pd.concat([X_traditional_independent, X_traditional_independent_test])\n",
    "    y_traditional_independent = pd.concat([y_traditional_independent, y_traditional_independent_test])\n",
    "   \n",
    "    # next we load the traditional model\n",
    "    print(traditional_model_path)\n",
    "    print(independent_traditional_path)\n",
    "    model = joblib.load(traditional_model_path+'model.pkl')\n",
    "\n",
    "    X_trained_original = pd.read_parquet(original_traditional_path +'X_train_time_point_(\\'random\\', 1)_lookback_'+str(lookback)+'.parquet')\n",
    "    X_traditional_independent[list(set(X_trained_original.columns).difference(set(X_traditional_independent.columns)))] = 0\n",
    "    X_traditional = X_traditional_independent[X_trained_original.columns]\n",
    "\n",
    "    # calculate test set predictions\n",
    "    pred_test = pd.DataFrame(model.predict(X_traditional), columns=['pred'])\n",
    "    pred_proba_test = pd.DataFrame(model.predict_proba(X_traditional), columns=['False','True'])\n",
    "    test_gt_and_preds = pd.concat([y_traditional_independent.reset_index(drop=True), pred_test.reset_index(drop=True), pred_proba_test.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "\n",
    "    #display(test_gt_and_preds)\n",
    "    test_res = get_res_df(test_gt_and_preds)\n",
    "\n",
    "    #display(test_res)\n",
    "    test_gt_and_preds['seed'] = seed\n",
    "    test_gt_and_preds['database'] = database\n",
    "    test_res['seed'] = seed\n",
    "    test_res['database'] = database\n",
    "    return test_gt_and_preds, test_res\n",
    "\n",
    "\n",
    "datasets = ['mimic', 'eicu', 'pic']\n",
    "roc_data = {dataset: {'mean_fpr': np.linspace(0, 1, 100), 'tpr_list': [], 'auc_list': []} for dataset in datasets}\n",
    "\n",
    "all_test_res = pd.DataFrame()\n",
    "\n",
    "for dataset in datasets:\n",
    "    for seed in [42, 43, 44, 45, 46]: #\n",
    "        test_gt_and_preds, test_res = independent_test(database=dataset, seed=seed)\n",
    "        all_test_res = pd.concat([all_test_res, test_res])\n",
    "        #display(test_gt_and_preds)\n",
    "        gt = test_gt_and_preds['lot<5d']\n",
    "        pred_prob = test_gt_and_preds['True']\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(gt, pred_prob)\n",
    "        roc_data[dataset]['tpr_list'].append(np.interp(roc_data[dataset]['mean_fpr'], fpr, tpr))\n",
    "        roc_data[dataset]['tpr_list'][-1][0] = 0.0  # Startpunkt bei 0\n",
    "        roc_data[dataset]['auc_list'].append(auc(fpr, tpr)) \n",
    "\n",
    "for dataset in datasets:\n",
    "    mean_tpr = np.mean(roc_data[dataset]['tpr_list'], axis=0)\n",
    "    mean_tpr[-1] = 1.0  \n",
    "    mean_auc = np.mean(roc_data[dataset]['auc_list'])\n",
    "    std_auc = np.std(roc_data[dataset]['auc_list']) \n",
    "    std_tpr = np.std(roc_data[dataset]['tpr_list'], axis=0)\n",
    "\n",
    "    plt.plot(roc_data[dataset]['mean_fpr'], mean_tpr, label=f'{dataset} (AUC = {mean_auc:.2f} ± {std_auc:.2f})')\n",
    "    \n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    plt.fill_between(roc_data[dataset]['mean_fpr'], tprs_lower, tprs_upper, alpha=0.2)\n",
    "\n",
    "fs = 14\n",
    "#plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.legend(fontsize=fs)\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate', fontsize=fs)\n",
    "plt.xlabel('False Positive Rate', fontsize=fs)\n",
    "plt.xticks(fontsize=fs)\n",
    "plt.yticks(fontsize=fs)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/experiments/transferability/traditional_transferability_auroc.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_res.rename({'balanced_accuracy': 'Balanced Accuracy', 'prc_auc':'AUPRC', 'roc_auc':'AUROC', 'f1':'F1', 'recall':'Recall', 'precision':'Precision'}, inplace=True, axis=1)\n",
    "mean_measurements = all_test_res.groupby('database').mean().reset_index()\n",
    "std_measurements = all_test_res.groupby('database').std().reset_index()\n",
    "metrics = mean_measurements.columns.difference(['database', 'seed'])\n",
    "df_combined = pd.DataFrame(index=mean_measurements['database'], columns=pd.MultiIndex.from_product([metrics, ['mean', 'std']]))\n",
    "for metric in metrics:\n",
    "    df_combined[(metric, 'mean')] = mean_measurements.set_index('database')[metric]\n",
    "    df_combined[(metric, 'std')] = std_measurements.set_index('database')[metric]\n",
    "\n",
    "df_combined.reset_index() \n",
    "display(df_combined)\n",
    "print(df_combined[['Balanced Accuracy', 'AUPRC', 'AUROC']].to_latex(float_format=\"%.2f\", bold_rows=True, caption='Transferarbility performance of traditional model'))\n",
    "print(df_combined[['Precision', 'Recall', 'F1']].to_latex(float_format=\"%.2f\", bold_rows=True, caption='Transferarbility performance of traditional model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_measurements = all_test_res.groupby('database').mean().reset_index()\n",
    "std_measurements = all_test_res.groupby('database').std().reset_index()\n",
    "\n",
    "# Melt the DataFrames to have the metric names as a single column, which is required for seaborn plotting\n",
    "mean_melted = mean_measurements.melt(id_vars='database', var_name='metric', value_name='mean')\n",
    "std_melted = std_measurements.melt(id_vars='database', var_name='metric', value_name='std')\n",
    "\n",
    "# Merge the mean and std DataFrames on model and metric to have a single DataFrame for plotting\n",
    "merged_measurements = pd.merge(mean_melted, std_melted, on=['database', 'metric'])\n",
    "\n",
    "# Loop over each metric and create a separate plot for it\n",
    "for metric in merged_measurements['metric'].unique():\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    metric_data = merged_measurements[merged_measurements['metric'] == metric]\n",
    "    display(metric_data)\n",
    "    sns.barplot(data=metric_data, x='database', y='mean',capsize=.1)\n",
    "    plt.errorbar(data=metric_data, x=range(len(metric_data)), y='mean', yerr=metric_data['std'], fmt='none', c='black', capsize=5)\n",
    "    plt.title(f'{metric}')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('Mean Value and Standard Deviation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "pr_data = {dataset: {'recall': np.linspace(0, 1, 100), 'precision_list': [], 'ap_list': []} for dataset in datasets}\n",
    "\n",
    "#positive_class_frequency = 0\n",
    "positive_class_frequency = {\n",
    "                                'mimic': 0,\n",
    "                                'eicu': 0,\n",
    "                                'pic': 0,\n",
    "                            }\n",
    "\n",
    "seeds = [42, 43, 44, 45, 46]\n",
    "for dataset in datasets:\n",
    "    for seed in seeds:\n",
    "        test_gt_and_preds, test_res = independent_test(database=dataset, seed=seed)\n",
    "        #display(test_gt_and_preds)\n",
    "        gt = test_gt_and_preds['lot<5d']\n",
    "        pred_prob = test_gt_and_preds['True']\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(gt, pred_prob)\n",
    "        pr_data[dataset]['precision_list'].append(np.interp(pr_data[dataset]['recall'], recall[::-1], precision[::-1]))\n",
    "        pr_data[dataset]['ap_list'].append(average_precision_score(gt, pred_prob))\n",
    "        positive_class_frequency[dataset] += gt.mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    mean_precision = np.mean(pr_data[dataset]['precision_list'], axis=0)\n",
    "    mean_ap = np.mean(pr_data[dataset]['ap_list'])\n",
    "    std_ap = np.std(pr_data[dataset]['ap_list'])\n",
    "    std_precision = np.std(pr_data[dataset]['precision_list'], axis=0)\n",
    "\n",
    "    plt.plot(pr_data[dataset]['recall'], mean_precision, label=f'{dataset} (AP = {mean_ap:.2f} ± {std_ap:.2f})')\n",
    "    \n",
    "\n",
    "    precision_upper = np.minimum(mean_precision + std_precision, 1)\n",
    "    precision_lower = np.maximum(mean_precision - std_precision, 0)\n",
    "    plt.fill_between(pr_data[dataset]['recall'], precision_lower, precision_upper, alpha=0.2)\n",
    "\n",
    "\n",
    "#positive_class_frequency /= len(seeds) * len(models)\n",
    "display(positive_class_frequency)\n",
    "positive_class_frequency['mimic'] = positive_class_frequency['mimic'] / (len(seeds))\n",
    "positive_class_frequency['eicu'] = positive_class_frequency['eicu'] / (len(seeds))\n",
    "positive_class_frequency['pic'] = positive_class_frequency['pic'] / (len(seeds))\n",
    "display(positive_class_frequency)\n",
    "plt.hlines(positive_class_frequency['mimic'], 0, 1, colors='blue', linestyles='dashed', label='chance level (mimic)')\n",
    "plt.hlines(positive_class_frequency['eicu'], 0, 1, colors='orange', linestyles='dashed', label='chance level (eicu)')\n",
    "plt.hlines(positive_class_frequency['pic'], 0, 1, colors='green', linestyles='dashed', label='chance level (pic)')\n",
    "\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('Precision', fontsize=fs)\n",
    "plt.xlabel('Recall', fontsize=fs)\n",
    "plt.xticks(fontsize=fs)\n",
    "plt.yticks(fontsize=fs)\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/experiments/transferability/traditional_transferability_auprc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# next day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_res = pd.DataFrame()\n",
    "for dataset in datasets:\n",
    "    for seed in [42, 43, 44, 45, 46]:\n",
    "        test_res = pd.read_parquet(\"experiments/transferability/test_res_\"+dataset+\"_\"+str(seed)+\".parquet\")\n",
    "        all_test_res = pd.concat([all_test_res, test_res])\n",
    "all_test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_res.rename({'balanced_accuracy': 'Balanced Accuracy', 'prc_auc':'AUPRC', 'roc_auc':'AUROC', 'f1':'F1', 'recall':'Recall', 'precision':'Precision'}, inplace=True, axis=1)\n",
    "mean_measurements = all_test_res.groupby('database').mean().reset_index()\n",
    "std_measurements = all_test_res.groupby('database').std().reset_index()\n",
    "metrics = mean_measurements.columns.difference(['database', 'seed'])\n",
    "df_combined = pd.DataFrame(index=mean_measurements['database'], columns=pd.MultiIndex.from_product([metrics, ['mean', 'std']]))\n",
    "for metric in metrics:\n",
    "    df_combined[(metric, 'mean')] = mean_measurements.set_index('database')[metric]\n",
    "    df_combined[(metric, 'std')] = std_measurements.set_index('database')[metric]\n",
    "\n",
    "df_combined.reset_index() \n",
    "display(df_combined)\n",
    "print(df_combined[['Balanced Accuracy', 'AUPRC', 'AUROC']].to_latex(float_format=\"%.2f\", bold_rows=True, caption='Transferarbility performance of next day model'))\n",
    "print(df_combined[['Precision', 'Recall', 'F1']].to_latex(float_format=\"%.2f\", bold_rows=True, caption='Transferarbility performance of next day model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['mimic', 'eicu', 'pic']\n",
    "roc_data = {dataset: {'mean_fpr': np.linspace(0, 1, 100), 'tpr_list': [], 'auc_list': []} for dataset in datasets}\n",
    "\n",
    "all_test_res = pd.DataFrame()\n",
    "\n",
    "for dataset in datasets:\n",
    "    for seed in [42, 43, 44, 45, 46]: #\n",
    "        #test_gt_and_preds, test_res = independent_test_nd(database=dataset, seed=seed)\n",
    "        test_gt_and_preds = pd.read_parquet(\"experiments/transferability/test_gt_and_preds_\"+dataset+\"_\"+str(seed)+\".parquet\")\n",
    "        test_res = pd.read_parquet(\"experiments/transferability/test_res_\"+dataset+\"_\"+str(seed)+\".parquet\")\n",
    "\n",
    "        #all_test_res = pd.concat([all_test_res, test_res])\n",
    "        # #display(test_gt_and_preds)\n",
    "        gt = test_gt_and_preds['next_day']\n",
    "        pred_prob = test_gt_and_preds['True']\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(gt, pred_prob)\n",
    "        roc_data[dataset]['tpr_list'].append(np.interp(roc_data[dataset]['mean_fpr'], fpr, tpr))\n",
    "        roc_data[dataset]['tpr_list'][-1][0] = 0.0  \n",
    "        roc_data[dataset]['auc_list'].append(auc(fpr, tpr)) \n",
    "\n",
    "for dataset in datasets:\n",
    "    mean_tpr = np.mean(roc_data[dataset]['tpr_list'], axis=0)\n",
    "    mean_tpr[-1] = 1.0  \n",
    "    mean_auc = np.mean(roc_data[dataset]['auc_list'])\n",
    "    std_auc = np.std(roc_data[dataset]['auc_list']) \n",
    "    std_tpr = np.std(roc_data[dataset]['tpr_list'], axis=0)\n",
    "\n",
    "    plt.plot(roc_data[dataset]['mean_fpr'], mean_tpr, label=f'{dataset} (AUC = {mean_auc:.2f} ± {std_auc:.2f})')\n",
    "    \n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    plt.fill_between(roc_data[dataset]['mean_fpr'], tprs_lower, tprs_upper, alpha=0.2)\n",
    "\n",
    "#plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.legend(fontsize=fs)\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate', fontsize=fs)\n",
    "plt.xlabel('False Positive Rate', fontsize=fs)\n",
    "plt.xticks(fontsize=fs)\n",
    "plt.yticks(fontsize=fs)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/experiments/transferability/nd_transferability_auroc.png', format='png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "pr_data = {dataset: {'recall': np.linspace(0, 1, 100), 'precision_list': [], 'ap_list': []} for dataset in datasets}\n",
    "\n",
    "#positive_class_frequency = 0\n",
    "positive_class_frequency = {\n",
    "                                'mimic': 0,\n",
    "                                'eicu': 0,\n",
    "                                'pic': 0,\n",
    "                            }\n",
    "\n",
    "seeds = [42, 43, 44, 45, 46]\n",
    "for dataset in datasets:\n",
    "    for seed in seeds:\n",
    "        test_gt_and_preds = pd.read_parquet(\"experiments/transferability/test_gt_and_preds_\"+dataset+\"_\"+str(seed)+\".parquet\")\n",
    "        test_res = pd.read_parquet(\"experiments/transferability/test_res_\"+dataset+\"_\"+str(seed)+\".parquet\")\n",
    "\n",
    "        #display(test_gt_and_preds)\n",
    "        gt = test_gt_and_preds['next_day']\n",
    "        pred_prob = test_gt_and_preds['True']\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(gt, pred_prob)\n",
    "        pr_data[dataset]['precision_list'].append(np.interp(pr_data[dataset]['recall'], recall[::-1], precision[::-1]))\n",
    "        pr_data[dataset]['ap_list'].append(average_precision_score(gt, pred_prob))\n",
    "        positive_class_frequency[dataset] += gt.mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    mean_precision = np.mean(pr_data[dataset]['precision_list'], axis=0)\n",
    "    mean_ap = np.mean(pr_data[dataset]['ap_list'])\n",
    "    std_ap = np.std(pr_data[dataset]['ap_list'])\n",
    "    std_precision = np.std(pr_data[dataset]['precision_list'], axis=0)\n",
    "\n",
    "    plt.plot(pr_data[dataset]['recall'], mean_precision, label=f'{dataset} (AP = {mean_ap:.2f} ± {std_ap:.2f})')\n",
    "    \n",
    "    precision_upper = np.minimum(mean_precision + std_precision, 1)\n",
    "    precision_lower = np.maximum(mean_precision - std_precision, 0)\n",
    "    plt.fill_between(pr_data[dataset]['recall'], precision_lower, precision_upper, alpha=0.2)\n",
    "\n",
    "#positive_class_frequency /= len(seeds) * len(models)\n",
    "display(positive_class_frequency)\n",
    "positive_class_frequency['mimic'] = positive_class_frequency['mimic'] / (len(seeds) )\n",
    "positive_class_frequency['eicu'] = positive_class_frequency['eicu'] / (len(seeds) )\n",
    "positive_class_frequency['pic'] = positive_class_frequency['pic'] / (len(seeds))\n",
    "display(positive_class_frequency)\n",
    "plt.hlines(positive_class_frequency['mimic'], 0, 1, colors='blue', linestyles='dashed', label='chance level (mimic)')\n",
    "plt.hlines(positive_class_frequency['eicu'], 0, 1, colors='orange', linestyles='dashed', label='chance level (eicu)')\n",
    "plt.hlines(positive_class_frequency['pic'], 0, 1, colors='green', linestyles='dashed', label='chance level (pic)')\n",
    "plt.legend(fontsize=12)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('Precision', fontsize=fs)\n",
    "plt.xlabel('Recall', fontsize=fs)\n",
    "plt.xticks(fontsize=fs)\n",
    "plt.yticks(fontsize=fs)\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/experiments/transferability/nd_transferability_auprc.png', format='png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
